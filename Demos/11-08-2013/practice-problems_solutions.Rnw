\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,amsthm,fancyhdr,palatino}
\usepackage[textheight=8in,textwidth=6.5in]{geometry}

%%%%% knitr code to make sure things stay inside listings box:
\usepackage{listings}
\usepackage{inconsolata}

\lhead{STAT 135}
\chead{Practice Problems}
\rhead{Solutions}
\cfoot{\thepage}

\renewcommand{\L}{\mathcal{L}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\R}{\texttt{R}}
\newcommand{\N}{\ensuremath{\mathcal{N}}}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}

\title{STAT 135}
\author{Solutions to Practice Problems}
\date{}

\begin{document}
\maketitle
\pagestyle{fancy}

<<globalParameters,echo=FALSE>>=
set.seed(1234)
opts_chunk$set(comment="#",tidy=F,warning=FALSE,message=FALSE,highlight=FALSE)
library(ggplot2)
library(reshape2)
@

<<stayInside,echo=FALSE>>=
  options(width=60)
  listing <- function(x, options) {
     paste("\\begin{lstlisting}[basicstyle=\\ttfamily,breaklines=true]\n", x,"\\end{lstlisting}\n", sep = "")
  }
  
  knit_hooks$set(source=listing, output=listing)
@

\paragraph{\#7.7.7} Suppose that a simple random sample is used to estimate the proportion of families in a certain area that are living below the poverty level. If this proportion is roughly 0.15, what sample size is necessary so that the standard error of the estimate is 0.02?

\begin{proof}[Solution]
From the table on page 214: if the parameter to be estimated in the \textit{proportion} of families living in poverty, $p$, then the sample proportion, $\hat{p}$ is our natural estimate. Furthermore, 
\[
  \sigma_{\hat{p}}^2(n,N) = \dfrac{p(1-p)}{n}\left(\dfrac{N-n}{N-1}\right)
\]
So, if we solve $\sigma_{\hat{p}}^2(n,N) = 0.02^2$, we have
\[
  \frac{0.15\times 0.85}{n}\dfrac{N-n}{N-1} = 0.004 \Longleftrightarrow n = \dfrac{1275 N}{1271 +4N}
\]
However, if you ignore the finite population correction term, and assume $N \gg n$, 
\[
  n \approx \dfrac{1275}{4} = \Sexpr{round(1275/4,digits=0)}
\]
\end{proof}

\paragraph{\#7.7.9} In a simple random sample of 1,500 voters, 55\% said they planned to vote for a particular proposition, and 45\% said they planned to vote against it. The estimated margin of victory for the proposition is thus 10\%. What is the standard error of this estimated margin? What is an approximate 95\% confidence interval for the margin?

\begin{proof}[Solution]
Consider each polled voter, $X_i$, where
\[
  X_i = \begin{cases} 1 &\text{ if they are in favor} \\ 0 &\text{ if they are against} \end{cases}
\]
If the true proportion of voters who are in favor of the proposition is $p$, then the margin of victory for the proposition is
\[
  m = p - (1-p) = 2p -1
\]
Hence, we may estimate $m$ via
\[
  \hat{m} = 2\hat{p} - 1
\]
where $\hat{p} = \Xbar$. It then follows that 
\[
  \sigma^2_{\hat{m}} = \var(2\hat{p}-1) = 4\var(\hat{p}) = 4\sigma_{\hat{p}}^2 = \dfrac{4p(1-p)}{n-1} \times \dfrac{N-n}{N-1}
\]
but if we ignore the finite population correction and assume that $N \gg n$, we have that 
\[
  \sigma^{2}_{\hat{m}} \approx \dfrac{4\hat{p}(1-\hat{p})}{n-1} = \Sexpr{p.hat <- 0.55; var.p.hat <- 4*p.hat*(1-p.hat)/1499; var.p.hat}
\]
Which yields an (approximate) standard error of $\Sexpr{sqrt(var.p.hat)}$.

To get an approximate 95\% confidence interval, we'll consider a central limit type of argument. Why?/How? Well, we know (from page 215) that for $n$ large, but still small relative to $N$, $\hat{p} = \Xbar$ is approximately normal. That is, $\hat{p} \stackrel{approx}{\sim} N(p,p(1-p)/n)$. Thus,
  \[
    \hat{m} \stackrel{approx}{\sim} N(m=2p-1, 4p(1-p)/n)
  \]
So, to make our confidence interval, we play around with CDF's:
\begin{align*}
  P( -\delta \leq Z  \leq \delta) 
  &\approx P\left(-\delta \leq \frac{\hat{m} - m}{\sqrt{4p(1-p)/n}} \leq \delta \right)  \\
  &\approx P\left(-\delta \leq \frac{\hat{m} - m}{\sqrt{4{\hat{p}}(1-\hat{{p}})/n}} \leq \delta \right)  \\
  &= P\left( -\sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \leq \hat{m} - m \leq \sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \right) \\
  &= P\left( \sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \geq m - \hat{m} \geq -\sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \right) \\
  &= P\left( \hat{m} + \sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \geq m \geq \hat{m} -\sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \right) \\
  &= P\left( m \in \left[{\hat{m}} - \sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta,\hat{{m}} + \sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \right]\right)
\end{align*}
So, we just need to find $\delta$ such that $P(|Z| \leq \delta) = 0.95$, but
\[
0.95 = P(|Z| \leq \delta) = \Phi(\delta)-\Phi(-\delta) = 2\Phi(\delta) - 1 \Longleftrightarrow 0.975 = \Phi(\delta) \Longleftrightarrow \delta = \Sexpr{delta <- qnorm(0.975); delta}
\]
Thus,
\[
\left[{\hat{m}} - \sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta,\hat{{m}} + \sqrt{\dfrac{4\hat{p}(1-\hat{p})}{n}}\delta \right] = [\Sexpr{2*p.hat-1 +c(-1,1)*delta*sqrt(4*p.hat*(1-p.hat)/1499)}]
\]
\end{proof}

\paragraph{\#7.7.17} A 90\% confidence interval for the average number of children per household, based on a simple random sample is found to be (0.7,2.1). Can we conclude that 90\% of households have between 0.7 and 2.1 children?

\begin{proof}[Solution]
No. A confidence interval is a bit more subtle than that. When we report a confidence interval like
\[
  P\left[\theta \in \left(L(\mathbf{X}), U(\mathbf{X})\right)\right] = (1-\alpha)
\]
we're making a statement about two random variables, $L(\X)$, the lower bound of the interval, and $U(\X)$, the upper bound. From this mathematical expression, we say that $100(1-\alpha)$\% of the time, these two random variables make an interval that capture the parameter of interest. 

Just like the sample mean of a particular set of data is a realization of the random variable $\Xbar$, the reported confidence interval is a realization of the random interval, $(L(\X), U(\X))$. 
\end{proof}

\paragraph{\#7.7.37} Two surveys were independently conducted to estimate a population mean, $\mu$. Denote the estimates and their standard errors by $\Xbar_1$ and $\Xbar_2$, and $\sigma_{\Xbar_1}$ and $\sigma_{\Xbar_2}$. Assume that $\Xbar_1$ and $\Xbar_2$ are unbiased. For some $\alpha$ and $\beta$, the two estimates can be combined to make a better estimator:
  \[
    X = \alpha \Xbar_1 + \beta \Xbar_2
  \]
\begin{enumerate}
  \item[a)] Find the conditions on $\alpha$ and $\beta$ that make the combined estiamte unbiased.
  \item[b)] What choice of $\alpha$ and $\beta$ minimizes the variances, subject to the condition of unbiasedness?
\end{enumerate}

\begin{proof}[Solution]
For the unbiasedness, observe that
  \begin{align*}
    E(X) &= E(\alpha\Xbar_1 + \beta\Xbar_2) \\
         &= \alpha E(\Xbar_1) + \beta E(\Xbar_2) \\
         &= \alpha \mu + \beta \mu \\
         &= \mu(\alpha + \beta)
  \end{align*}
  So $X$ is unbiased \textit{if and only if} $\alpha + \beta = 1$. 

Now, what is the variance of our combined estimate?
  \begin{align*}
    \var(X) &= \var(\alpha\Xbar_1 + \beta\Xbar_2) \\
           &= \alpha^2\var(\Xbar_1) + \beta^2\var(\Xbar_2) + 2\alpha\beta \cov(\Xbar_1, \Xbar_2) \\
           &= \alpha^2 \sigma_{\Xbar_1}^2 + \beta^2 \sigma_{\Xbar_2}^2
  \end{align*}
  where we were able to drop the covariance term since we were told that the surveys were independently conducted, therefore functions of their data should also be independent. Now, if we are use the constraint that $E(X) = \mu$, then we can replace $\alpha^2$ above with $(1-\beta)^2$. Hence, 
  \begin{align*}
    \var(X) &= (1-\beta)^2 \sigma_{\Xbar_1}^2 + \beta^2 \sigma_{\Xbar_2}^2 \\
          &= (1-2\beta + \beta^2) \sigma_{\Xbar_1}^2 + \beta^2 \sigma_{\Xbar_2}^2 \\
           &= (1-2\beta)\sigma_{\Xbar_1}^2 + \beta^2 \left(\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2\right) 
  \end{align*}
  Thus, we may now think of $\var(X) = f(\beta)$, and optimizing it can be done with simple calculus (this isn't the only way to do this, though). 
  \begin{align*}
    \dfrac{d}{d\beta}\var(X) &= 0 \Longleftrightarrow  -2\sigma_{\Xbar_1}^{2} + 2\beta \left(\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2\right) = 0 \Longleftrightarrow \beta = \dfrac{\sigma_{\Xbar_1}^2 }{\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2} \\
    \dfrac{d^2}{d\beta^2}\var(X) &= 2\left(\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2\right) > 0
  \end{align*}
  This demonstrates that our critical point is a global minimum, which means
  \begin{align*}
    \alpha &= \dfrac{\sigma_{\Xbar_2}^2 }{\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2} \\
    \beta &= \dfrac{\sigma_{\Xbar_1}^2 }{\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2}
  \end{align*}
  minimze the variance of $X$. 
  
  Note, that since
  \[
    \sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2 > \max(\sigma_{\Xbar_1}^2,\sigma_{\Xbar_2}^2) \Longleftrightarrow \frac{1}{\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2} < \frac{1}{\max(\sigma_{\Xbar_1}^2,\sigma_{\Xbar_2}^2)}
  \]
  it follows that these particular values of $\alpha$ and $\beta$, yield
  \[
    \var(X) = \dfrac{\sigma_{\Xbar_1}^2 \sigma_{\Xbar_2}^2}{\sigma_{\Xbar_1}^2 + \sigma_{\Xbar_2}^2} < \frac{\sigma_{\Xbar_1}^2 \sigma_{\Xbar_2}^2}{\max(\sigma_{\Xbar_1}^2,\sigma_{\Xbar_2}^2)} < \min(\sigma_{\Xbar_1}^2,\sigma_{\Xbar_2}^2)
  \]
  which is pretty cool. If you think about it, it means that we've found a way to take two estimates, and make a less variable (i.e. better) estimator from them.
\end{proof}

\paragraph{\#8.10.17} Suppose that $X_1, \ldots X_n$ are iid on the interval $[0,1]$ with density function
\[
  f(x;\alpha) = \frac{\Gamma(2\alpha)}{\Gamma(\alpha)^2} [x(1-x)]^{\alpha-1}
\]
where $\alpha > 0$ is a parameter to be estimated from the sample. (Note, this implies that $X_i \sim {\beta}eta(\alpha=\beta)$) It can be shown that
\begin{align*}
  E(X) &= \frac{1}{2} \\
  \var(X) &= \frac{1}{4(2\alpha+1)}
\end{align*}
\begin{enumerate}
  \item[a)] How does the shape of the density depend on $\alpha$?
  \item[b)] How can the method of moments be used to estimate $\alpha$?
  \item[c)] What equation does the mle of $\alpha$ satisfy?
  \item[d)] What is the asymptotic variance of the mle?
\end{enumerate}

\begin{proof}[Solution]
To understand the effect of $\alpha$ on the shape, you can use calculus (if you know the gamma function) and take derivatives. Or, you can pick a couple sample alpha and plot the density with a computer package. See figure 1.
  <<effectOfAlpha,echo=FALSE,dev='pdf',out.width="0.85\\textwidth",fig.align='center',fig.cap="Problem 8.17.a",fig.pos="ht!">>=
  require(reshape2)
  require(ggplot2)
  alpha <- seq(from=0.5,to=8,length.out=12)
  densitY <- function(alpha) {
    function(x) {gamma(2*alpha)/(gamma(alpha)^2)*(x*(1-x))^(alpha-1)}
  }
  domain <- data.frame(X=seq(from=0,to=1,length.out=500))
  densitY.profiles <- with(domain, {
    sapply(X=alpha,FUN=function(alpha){densitY(alpha)(X)})
  })
  colnames(densitY.profiles) <- paste("alpha=",alpha,sep="")
  domain <- cbind(domain,densitY.profiles)

  melted.df <- melt(domain,id.vars=c("X"))
  ggplot(data=melted.df) + geom_line(aes(x=X,y=value,color=variable)) + labs(y=expression(paste("f(x ; ",alpha,")",sep="")),x=expression(x))
  @

The method of moments estimator can be used via the following manipulation:
\begin{align*}
  & E(X^2) - E(X)^2 = \var(X) = \dfrac{1}{4(2\alpha+1)} \\
  &\Longleftrightarrow \frac{2\mu_{2} - 1}{2} = \frac{1}{4(2\alpha+1)} \\
  &\Longleftrightarrow 2(2\alpha + 1) = \frac{1}{2\mu_{2}-1} \\
  &\Longleftrightarrow \alpha = \frac{1}{8\mu_{2}-4} - \frac{1}{2}
\end{align*}
  So, $\hat{\alpha} = \frac{1}{8\hat{\mu}_{2}-4} - \frac{1}{2}$, is our MoM estimator.
  Via straight calculus, we can find the answer to b):
  \begin{align*}
    \L(\alpha) &= \prod_{i=1}^{n} f(x_i;\alpha) \\
                &= \left(\frac{\Gamma(2\alpha)}{\Gamma(\alpha)^2}\right)^{n} \left(\prod_{i=1}^{n} x_i(1-x_i)\right)^{\alpha-1} \\
   \Rightarrow \ell(\alpha) &= n\left( \log(\Gamma(2\alpha)) - 2 \log(\Gamma(\alpha)) \right) + (\alpha-1)\sum_{i=1}^{n}\log(x_i(1-x_i))                
  \end{align*}
  Thus, the mle must satisfy
  \[
  \dfrac{d\ell}{d\alpha} = 0 \Leftrightarrow 2n \left[\dfrac{\Gamma'(2\alpha)}{\Gamma(2\alpha)} - \dfrac{\Gamma'(\alpha)}{\Gamma(\alpha)}\right] + \sum_{i=1}^{n}\log(x_i(1-x_i)) = 0
  \]
  The asymptotic variance of the mle is the inverse of the fisher information, which is calculated via $I(\alpha) = E(-\ell''(\alpha))$. Since 
  \[
  I(\alpha) = E\left( - \ell''(\alpha) \right) = E\left( - 2n \left[\dfrac{\Gamma'(2\alpha)}{\Gamma(2\alpha)} - \dfrac{\Gamma'(\alpha)}{\Gamma(\alpha)}\right] \right) = 2n \left[\dfrac{\Gamma'(\alpha)}{\Gamma(\alpha)} -\dfrac{\Gamma'(2\alpha)}{\Gamma(2\alpha)} \right]
  \]
  it follows that the asymptotic variance is
  \[
   \frac{1}{2n} \left[\dfrac{\Gamma'(\alpha)}{\Gamma(\alpha)} -\dfrac{\Gamma'(2\alpha)}{\Gamma(2\alpha)} \right]^{-1}
  \]
\end{proof}
\paragraph{\#8.10.21} Suppose that $X_1, \ldots, X_n$ are iid with density function
\[
  f(x;\theta) = e^{-(x-\theta)} I(x \geq \theta)
\]
  \begin{enumerate}
  \item[a)] Find the method of moments estimate of $\theta$.
  \item[b)] Find the mle of $\theta$. (Hint: be careful and don't differentiate before thinking. For what values of $\theta$ is the likelihood positive?)
  \end{enumerate}
\begin{proof}[Solution]
  There are a few ways to do this, one way is to recognize that if $Y \sim Exp(\lambda=1)$, then $f_{Y}(y) = e^{-y} I(y \geq 0)$. So, we can think of $X$ as just a shifted version of $Y$. In particular, $f_{X}(x) = f_{Y}(x-\theta)$, so $X = Y + \theta$. Why?/How? Well, let $b \geq \theta$, then
  \begin{align*}
    P(X \leq b) &= \int_{\theta}^{b} f_{X}(x) \, dx \\
                &= \int_{\theta}^{b} f_{Y}(x-\theta) \, dx \\
                &\stackrel{y=x-\theta}{=} \int_{0}^{b-\theta} f_{Y}(y) \, dy \\
                &= P(Y \leq b - \theta)
  \end{align*}
  Hence, $P(X \leq b) = P(Y \leq b - \theta) = P(Y + \theta \leq b)$. Which means that the distribution functions for $X$ and $Y+\theta$ agree on where ever they're defined. This shows that the random variables are the same.
  Thus, 
  \begin{align*}
  E(X) &= E(Y + \theta) =  E(Y) + \theta = 1 + \theta, \\
  var(X) &= var(Y+\theta) = var(Y) = 1
  \end{align*}
  which means that $\Xbar - 1 = \hat{\theta}_{MoM}$. 
  
  For the MLE, we'll start as we always do: with the likelihood function.
  \begin{align*}
    \L(\theta) &= \prod_{i=1}^{n} f_{X}(x_i;\theta) \\
               &= \prod_{i=1}^{n} \exp(-(x_i-\theta)) I(x_i \geq \theta) \\
               &= \exp\left(-\sum_{i=1}^{n}(x_i-\theta)\right) I(\min(x_i) \geq \theta)
  \end{align*}
  So, immediately, we need to assume that $\theta \leq \min(x_i)$, otherwise there's nothing to maximize. Making this assumption, our log-likelihood is
  \[
  \ell(\theta;\X) = n\theta - n\Xbar
  \]
  which is a linear function of $\theta$, and grows as $\theta$ grows. This implies that the maximizer of our log-likelihood occurs at the right-most boundary point of its domain, $\min(x_i)$. Thus,
  \[
    \hat{\theta}_{mle}(\X) = \min(X_i)
  \]
  Note, it's not hard to show that the distribution of this estimator has density
  \[
    f(z;\theta) = n \exp\{-(n-1)(z-\theta)\} I(z \geq \theta)
  \]
\end{proof}
\paragraph{\#8.10.53} Let $X_1, \ldots, X_n$ be i.i.d. uniform on $[0,\theta]$. 
\begin{enumerate}
  \item[a)] Find the method of moments estimate of $\theta$ and its mean and variance.
  \item[b)] Find the mle of $\theta$.
  \item[c)] Find the probability density of the mle, and calculate its mean and variance.  Compare the variacne, the bias, and the mean squared error to those of the method of moments estimate.
  \item[d)] Find a modification of the mle that renders it unbiased.
\end{enumerate}

\begin{proof}[Solution]
  \begin{enumerate}
    \item[a)] First, recall that for $X \sim Unif(a,b)$, $E(X) = (a+b)/2$. Hence, if our $X_i \stackrel{iid}{\sim} Unif(0,\theta)$, $E(X) = \theta/2$. Since, the method of moments replaces $E(X)$ with $\Xbar$, and $\theta$ with $\hat{\theta}_{MoM}$, we solve for our estimator and get
    \[
      \hat{\theta}_{MoM} = 2 \Xbar
    \]
  Since our estimator is just a scaled sample mean (taken from an i.i.d), we can calculate its expectation and variance simply:
  \begin{align*}
    E(\hat{\theta}_{MoM}) &= E(2\Xbar) = 2 E(\Xbar) = 2 E(X_1) = \theta \\
    \var(\hat{\theta}_{MoM}) &= 4 \var(\Xbar) = \frac{4}{n}\var(X_1) = \frac{4\theta^2}{12 n} = \frac{\theta^2}{3n}
  \end{align*}
  \item[b)] For the mle, consider the likelihood
  
  \begin{align*}
    \L(\theta) &= \prod_{i=1}^{n} f_{X}(x_i \mid \theta) \\
               &= \prod_{i=1}^{n} \frac{I(x_i \in [0,\theta])}{\theta} \\
               &= \dfrac{I(0 \leq \min(x_i) \leq \max(x_i) \leq \theta)}{\theta^n}
  \end{align*}
  So, suppose $\theta \geq \max(x_i)$, then $\L(\theta) = \theta^{-n}$ is a monotonically decreasing function of $\theta$. Hence, it is maximimized at the left-most boundary point of its domain. In this instance, we're considering $\max(x_i) \leq \theta < \infty$, so this boundary point is $\hat{\theta}_{mle} = \max(x_i)$. 
  \item[c)] For the probability density, let $M(\X) = \hat{\theta}_{mle}(\X)$ and let's check out $F_{M}(x) = P( M(\X) \leq x)$. If $x \in [0,\theta]$:
  \newcommand{\indp}{\perp\!\!\!\perp}
  \begin{align*}
    F_{M}(x) &= P(M(\X) \leq x) \\
              &= P( X_1 \leq x, X_2 \leq x, \ldots, X_n \leq x) \\
              &\stackrel{X_i \indp X_j}{=} P(X_1 \leq x)^n \\
              &= \left(\frac{x}{\theta}\right)^n
  \end{align*}
  Hence, 
  \[
    f_{M}(x) = \dfrac{d}{dx}F_{M}(x) = n\left(\frac{x}{\theta}\right)^{n-1} I(x \in [0,\theta])
  \]
  To calculate the mean and variance, we'll use a trick:
  \begin{align*}
    E(M(\X)) &= \int_{0}^{\theta} x f_{M}(x) \, dx \\
      &= \int_{0}^{\theta} x n \left(\frac{x}{\theta}\right)^{n-1} \, dx \\
      &\stackrel{u=x/\theta}{=} \int_{0}^{1} \theta u (n u)^{n-1} \, du \\
      &= \theta \int_{0}^{1} u f_{Y}(u) \, du \\
      &= \theta E(Y)
  \end{align*}
  Where $Y \sim {\beta}eta(\alpha=n,\beta=1)$. Hence, $E(M(\X)) = \theta n / (n+1)$. Similarly,
  \begin{align*}
    E(M(\X)^2) &= \int_{0}^{\theta} x^2 f_{M}(x) \, dx \\
      &= \int_{0}^{\theta} x^2 n \left(\frac{x}{\theta}\right)^{n-1} \, dx \\
      &\stackrel{u=x/\theta}{=} \int_{0}^{1} (\theta u)^2 (n u)^{n-1} \, du \\
      &= \theta^2 \int_{0}^{1} u^2 f_{Y}(u) \, du \\
      &= \theta^2 E(Y^2)
    \end{align*}
    Thus,
    \begin{align*}
    \var(M(\X)) &= E(M(\X)^2) - E(M(\X))^2 \\
                &= \theta^2 E(Y^2) - \theta^2 E(Y)^2 \\
                &= \theta^2 \var(Y) \\
                &= \frac{\theta^2 n}{(n+1)^2(n+2)}
    \end{align*}
    Hence, using the formula $MSE = \var + bias^2$, we have
    \[
      MSE(M(\X)) = \frac{\theta^2 n}{(n+1)^2(n+2)} + \left(\dfrac{\theta n}{n+1} - \theta \right) = \dfrac{2\theta^2}{(n+1)(n+2)}
    \]
    Whereas,
    \[
      MSE(\hat{\theta}_{MoM}) = \var(\hat{\theta}_{MoM}) = \frac{\theta^2}{3n}
    \]
    So, the MSE for our mle decays by an order of magnitude faster than that of the method of moments estimator. (This is a very good thing!)
    \item[d)] Given that
    \[
      E(M(\X)) = \dfrac{\theta n }{n+1} \Longleftrightarrow \frac{n+1}{n} E(M(\X)) = \theta
    \]
    Hence $N(X) = (n+1)M(\X)/n$ is an unbiased estimator of $\theta$. 
  \end{enumerate}
\end{proof}

\paragraph{\#9.11.1} A coin is thrown independently 10 times to test the hypothesis that the probability of heads is $1/2$ versus the alternative that the probability is not $1/2$. The test rejects if either 0 or 10 heads are observed.
\begin{enumerate}
  \item[a)] What is the significance level of the test?
  \item[b)] If in fact the probability of heads is 0.1, what is the power of the test?
\end{enumerate}

\begin{proof}[Solution] Let's explicitly setup our hypotheses:
\[
H_0: \theta = \frac{1}{2}, \text{ versus } H_1: \theta\neq \frac{1}{2}
\]
Recall that in the situation of a composite hypothesis like $H_1 : \theta \neq 1/2$, we want to talk about a (general) power function:
\[
\pi(p) = P(\text{ reject $H_0$} \mid \theta = p) = P( \#H = 0,10 \mid \#H \sim Bin(10,p))
\]
The benefit to a definition like this, now, is that we can talk about significance and power through one item ($\pi(p)$). 

Since significance is just the chance of committing type-I error:
\[
\alpha = \pi(0.5) = P(\#H = 0,10 \mid \#H \sim Bin(10,0.5)) = \Sexpr{sum(dbinom(x=c(0,10),size=10,prob=0.5))}
\]
The power, if the true probability is 0.1, is just the value of the power function at this value:
\[
\beta = \pi(0.1) = P(\#H = 0,10 \mid \#H \sim Bin(10,0.1)) = \Sexpr{sum(dbinom(x=c(0,10),size=10,prob=0.1))}
\]
\end{proof}

\paragraph{\#9.11.6} Consider the coin tossing example of \S9.1. Suppose that instead of tossing the coin 10 times, the coin was tossed until a head came up and the total number of tosses, $X$, was recorded.
\begin{enumerate}
  \item[a)] If the prior probabilities are equal, which outcomes favor $H_0$ and which favor $H_1$.
  \item[c)] What is the significance level of a test that rejects $H_0$ if $X \geq 8$?
  \item[d)] What is the power of this test?
\end{enumerate}

\begin{proof}[Solution]
In this situation, the probability that the first coin yields a heads is $p_0 = 0.5$ and the second is $p_1 = 0.7$. Our test statistic, $X$ is now distributed according to a geometric distribution:
\[
P(X=k;p) = p(1-p)^{k-1}, k\in\{1,2,\ldots\}
\]
If $P(H_0) = P(H_1)$, then determining the outcomes which favor $H_0$ is equivalent to solving the following inequality:
\[
\dfrac{P(X=x\mid H_0)}{P(X=x\mid H_1)} > 1 \Leftrightarrow \dfrac{0.5^{x}}{0.7(0.3)^{x-1}} > 1 \Leftrightarrow \left(\dfrac{5}{3}\right)^{x} > \dfrac{7}{3} \Leftrightarrow x > \Sexpr{log(7/3)/log(5/3)}
\]
So, $x \geq \Sexpr{round(log(7/3)/log(5/3),digits=0)}$ would favor $H_0$ over $H_1$. Consequently, $x \leq \Sexpr{round(log(7/3)/log(5/3),digits=0)-1}$ would favor $H_1$ over $H_0$. 

A test that rejects if $X \geq 8$ would have significance level
\[
\alpha = P(X \geq 8 \mid H_0) = \Sexpr{pgeom(q=6,prob=0.5,lower.tail=F)}
\]
It would have power
\[
\beta = P(X \geq 8 \mid H_1) = \Sexpr{pgeom(q=6,prob=0.7,lower.tail=F)}
\]
And this low power makes sense: under the alternative we shouldn't have to wait long to see a heads. Thus, a rule that asks for 7 consecutive tails, is surely going to have a low rate of accepting the alternative.
\end{proof}

\paragraph{\#9.11.7} Let $X_1, \ldots, X_n \stackrel{iid}{\sim} Poisson(\lambda)$. Find the likelihood ratio for testing $H_0 : \lambda = \lambda_0$ versus $H_1 : \lambda = \lambda_1$, where $\lambda_1 > \lambda_0$. Use the fact that the sum of independent Poisson random variables follows a Poisson distribution to explain how to determine a rejection region for a test at level $\alpha$. 

\begin{proof}[Solution]
  Given our data, $\mathbf{x} = (x_1, \ldots, x_n)$, and a stochastic model of it, $\mathbf{X} = (X_1, \ldots, X_n)$, we write the likelihood ratio as
  \[
  LR(\mathbf{X}) = \dfrac{P(\mathbf{X}\mid H_0)}{P(\mathbf{X} \mid H_1)} = \dfrac{\prod_{i=1}^{n} e^{-\lambda_0} \dfrac{\lambda_0^{X_i}}{X_i!}}{\prod_{i=1}^{n} e^{-\lambda_1} \dfrac{\lambda_1^{X_i}}{X_i!}} = e^{-(\lambda_0 - \lambda_1)} \left(\dfrac{\lambda_0}{\lambda_1}\right)^{\sum_{i=1}^{n} X_i} = e^{-(\lambda_0 - \lambda_1)} \left(\dfrac{\lambda_0}{\lambda_1}\right)^{T(\mathbf{X})}
  \]
  The first thing we notice is that the condition, $\lambda_1 > \lambda_0$ imply that our likelihood shrinks \textit{if and only if} $T(\mathbf{X})$ grows. Thus, if we use the (general) rule that we reject if $T(\mathbf{X}) \geq c$, then we are performing a test that's equivalent to the Likelihood Ratio Test. 
  
  The value $c$ will determine the significance of this test. So, to find an appropriate value, we'll use the hint in the problem statement, and identify the null (and alternative) distribution of our test statistic as
  \[
  T(\mathbf{X}) \stackrel{H_i}{\sim} Poisson(n\lambda_i), \qquad i=0,1
  \]
  
  Hence, to create a test with significance level $\alpha \in (0,1)$, we find the value $c \in \mathbb{N}$ such that 
  \[
  P(T(\mathbf{X}) \geq c-1 \mid H_0) > \alpha \text{ and yet } P(T(\mathbf{X}) \geq c \mid H_0) \leq \alpha
  \]
  
  A concrete example of this would be for $n=3$, $\lambda_0 = 1$, and $\lambda_1 = 2$, then $T(\mathbf{X}) \stackrel{H_0}{\sim} Poisson(3)$, and 
  <<echo=FALSE>>=
  q <- 0:10
  tail.probs <- ppois(q=q-1,lambda=3,lower.tail=F)
  tbl <- t(as.matrix(tail.probs))
  rownames(tbl) <- c("P(T(X) >= x)")
  colnames(tbl) <- c("x = 0",1:10)
  tbl
  @
  Hence, if we wanted to limit our type-I error to being no greater than 5\%, we'd want to chose $c = 7$, and in this case, our exact level is \Sexpr{ppois(q=6,lambda=3,lower.tail=F)*100}\%.
\end{proof}

\paragraph{\#9.11.9} Let $X_1, \ldots, X_{25}$ be a sample from a normal distribution having a variance of 100. Find the rejection region for a test at level $\alpha = 0.10$ of $H_0 \mu = 0$ versus $H_{1}: \mu = 1.5$. What is the power of the test? Repeat for $\alpha = 0.01$. 

\begin{proof}[Solution] Since we're dealing with two, simple, hypotheses, the Neymann-Pearson lemma tells us that the likelihood ratio test is the most powerful test. So let's look at the likelihood ratio:

\begin{align*}
  LR(\X) &= \dfrac{P(\X \mid H_0)}{P(\X \mid H_1)} \\
         &= \dfrac{\displaystyle \prod_{i=1}^{25} f(X_i;\mu=0,\sigma=10)}{\displaystyle \prod_{i=1}^{25} f(X_i;\mu=1.5,\sigma=10)} \\
         &= \exp\left\{ -\frac{1}{200}\left(\sum_{i=1}^{25} X_i^2 - \sum_{i=1}^{25}(X_i-1.5)^2\right) \right\} \\
         &= \exp\left\{ -\frac{1}{200}\left(3\sum_{i=1}^{25}X_i - 25(1.5)^2\right) \right\} \\
         &= \exp\left\{ -\frac{1}{200}\left(3 T(\X) - \Sexpr{25*(1.5)^2}\right)\right\}
\end{align*}
So, the first thing we notice, here, is that for $T(\X)$ sufficiently large, our likelihood ratio behaves like $e^{-x}$ (which decreases as $x$ increases). This means that for some prescribed $\alpha \in (0,1)$, we can use $T(\X) \geq c_{\alpha}$ as an equivalent test with significance level $\alpha$.

To find $c_{\alpha}$, we recognize that 
\[
  T(\X) = \sum_{i=1}^{25} X_i \stackrel{H_i}{\sim}
  \begin{cases}
    N(\mu = 0, \sigma = 50) &\text{ if $i=0$} \\
    N(\mu = \Sexpr{25*1.5}, \sigma= 50) &\text{ if $i=1$}
  \end{cases}
\]
Hence 
  \begin{align*}
    \alpha &= P( T(\X) \geq c_{\alpha} \mid H_0) \\
           &= P( T(\X)/50 \geq c_{\alpha}/50 \mid H_0) \\
           &= P( Z \geq c_{\alpha}/50 ) \\
           &= 1 - \Phi(c_{\alpha}/50)
  \end{align*}
which implies that $c_{\alpha} = 50 \times z_{\alpha}$, where
  \[
    \Phi(z_{\alpha}) = 1-\alpha
  \]
  Using this result, we find $c_{0.1} = \Sexpr{50*qnorm(p=0.9)}$, and $c_{0.01} = \Sexpr{50*qnorm(p=0.99)}$. 
  The power for the test when $\alpha = 0.1$ is found via
  \begin{align*}
  \beta_{0.1} &= P( T(\X) \geq c_{0.1} \mid H_1) \\
        &= P\left( \dfrac{T(\X) - 37.5}{50} \geq \dfrac{50 z_{0.1} - 37.5}{50} \mid H_1 \right) \\
        &= P( Z \geq \Sexpr{q1 <- (50*qnorm(p=0.9)-37.5)/50; q1}) \\
        &= \Sexpr{pnorm(q=q1,lower.tail=F)}
  \end{align*}
  Similarly, the power for the test when $\alpha = 0.01$ is 
  \[
  \beta_{0.01} = \Sexpr{q2 <- (50*qnorm(p=0.99)-37.5)/50; pnorm(q=q2,lower.tail=F)}
  \]
  (Note there are discrepancies between my numbers and those in the back of the book, but there merely a consequence of rounding.)
\end{proof}

\paragraph{\#9.11.12} Let $X_1, \ldots, X_n$ be a random sample from an exponential distirbution with the density function $f(x; \theta) = \theta \exp[-\theta x]$. Derive a likelihood ratio test of $H_0 : \theta = \theta_0$ versus $H_1: \theta \neq \theta_0$, and show that the rejection region is of the form $\{ \Xbar \exp[-\theta_0 \Xbar] \leq c \}$. 

\begin{proof}[Solution]
  Since we're dealing with a two-sided alternative, we should beg to the \textit{Generalized} Likelihood Ratio test:
  \[
  GLR(\X) = \dfrac{\max_{\theta = \theta_0} P(\X;\theta)}{\max_{\omega \in \Omega} P(\X ; \theta)}
  \]
  while the numerator is simply $P(\X;\theta_0)$, we'll need to do some calculus to determine the denominator. 
  \begin{align*}
    P(\X;\theta) &= \prod_{i=1}^{n} f_{X}(X_i;\theta) = \theta^n \exp\left(-\theta\sum_{i=1}^{n}X_i\right) \\
    \Rightarrow \ell_n(\theta) &= \log(P(\X;\theta)) \\
        &= n\log(\theta) - \theta\sum_{i=1}^{n}X_i
  \end{align*}
  Hence,
  \[
  \ell_n'(\hat{\theta}) = 0 \Leftrightarrow \dfrac{n}{\hat{\theta}} - \sum_{i=1}^{n} X_i = 0 \Leftrightarrow \hat{\theta} = \dfrac{1}{\Xbar}
  \]
  and since $\ell_{n}''(\theta) = -n\theta^{-2} < 0$ for all $\theta$, it follows that our critical point, $\hat{\theta}$ is a global maximum. Thus,
  \begin{align*}
    GLR(\X) &= \dfrac{\theta_0^{n} e^{-n\theta_0\Xbar}}{\hat{\theta}^n e^{-n\hat{\theta}\Xbar}} \\
            &= \left(\dfrac{\theta_0}{\hat{\theta}}\right)^{n} \exp\left( -n\theta_0\Xbar + n\right) \\
            &= e^{n}\left(\theta_0 \Xbar e^{-\theta_0 \Xbar}\right)^{n} \\
            &= e^{n} \left( \theta_0 T(\X) \right)^{n} \\
            &= e^{n} \theta_{0}^{n} T(\X)^{n}
  \end{align*}
  where $T(\X) = \Xbar e^{-\theta_0 \Xbar} \geq 0 $. Recall, though, that the rule here is to reject when the GLR is small. That is,
  \[
  GLR(\X) \leq \lambda_{\alpha} \Longleftrightarrow e^{n}\theta_{0}^{n} T(\X)^{n} \leq \lambda_{\alpha} \Longleftrightarrow T(\X) \leq \sqrt[n]{e^{-n}\theta_{0}^{-n}\lambda_{\alpha}} = c_{\alpha}
  \]
\end{proof}

\paragraph{\#11.6.23} Let $X_1, \ldots, X_n \stackrel{iid}{\sim} F$ and $Y_1, \ldots, Y_{m} \stackrel{iid}{\sim} G$. The hypothesis to be tested is that $F=G$. Suppose for simplicity that $m+n$ is even so that in the combined sample of $X$'s and $Y$'s, $(m+n)/2$ observations are less than the median and $(m+n)/2$ are greater.
\begin{enumerate}
  \item[a)] As a test statistic, consider $T$, the number of $X$'s less than the median of the combined sample. Show that $T$ follows a hypergeometric distribution under the null hypothesis:
  \[
    P(T=t) = \dfrac{\displaystyle \binom{\frac{m+n}{2}}{t}\binom{\frac{m+n}{2}}{n-t}}{\displaystyle\binom{m+n}{n}}
  \]
  Explain how to form a rejection region for this test.
  \item[b)] Show how to find a confidence interval for the difference between the median of $F$ and the median of $G$ under the shift model, $G(x) = F(x-\Delta)$. (Hint: use the order statistics.)
  \item[c)] Apply the results (a) and (b) to the data of problem 21.
\end{enumerate}
\begin{proof}[Solution]
For starters, we should consider what are some of the implications of assuming that $F$ and $G$ are the same. Suppose we pooled the $X$'s and $Y$'s together, and and sorted them so that we had
\[
W_{(1)} \leq W_{(2)} \leq W_{(3)} \leq \cdots \leq W_{(m+n)}
\]
where $W_{(i)}$ is some observation from the $X$'s or the $Y$'s.

Now, if $F=G$, we have that the probability that $W_{(i)}$ came from the sample of $X$'s is no longer a function of its place in the ordering. All order statics have the same (unconditional) chance of being from the sample of $X$'s, $n/(m+n)$. Their conditional chance changes as we learn more, however. E.g. 
\[
  P(W_{(2)} \in \X \mid W_{(1)} \in \X) = \frac{n-1}{m+n}
\]

So, under the null-hypothesis, we may think of the location of each $X_i$ among the $W_{(i)}$ as a consequence of a draw, without replacement, from the set $\{1,2,\ldots, n+m\}$. This is precisely why our test statistic, under the null has a hypergeometric distribution.

Let's work through a concrete example. Say $n=10$, $m = 12$, so that $n+m=22$, and $(n+m)/2 = 11$, and we want to count the ways 6 $X$'s can lie below the sample median. First, we need to count the ways we can observe exactly 6 $X$'s below the sample median. We know that 11 of the pooled observations are below the median, which gives us 6 "slots" for our $X$'s to occupy. That is, there are $\binom{11}{6}$ ways for 6 $X$'s to be below the median. However, for every way to view 6 $X$'s below the median, we'd have to observe ($10-6 =$) 4 $X$'s above the median. There are $\binom{11}{4}$ ways to do that. Because we want to count all the ways we can see 6 below and 4 above, we should multiply these counts. That is, the total number of ways to observe exactly 6 $X$'s below is
\[
  \binom{11}{6} \times \binom{11}{4}
\]
To turn this into a probability, we need to count the total number of ways the $X$'s could have been "sprinkled" about the pooled collection. Again, back to our ``slots'' analogy. We have 22 total slots to hand out to each of the, 10, $X$'s so there are $\binom{22}{10}$ total configurations we could have observed. Thus,
\[
  P(T=6) = \dfrac{\binom{11}{6}\binom{11}{4}}{\binom{22}{10}}
\]
Generalizing this argument is tantamount to replacing ``11'' with $(m+n)/2$, ``6'' with $t$, and ``10'' with $n$.

To develop a rejection region for $T$, we need to we'd calculate tail probabilities of $T$ and find cut-off points in both tails that would yield a desired level. For instance, if we use the concrete example above, we have
  <<echo=FALSE>>=
  N <- 10; M <- 12 # |X| = N, |Y| = M
  left.tail.probs <- phyper(q=0:N,m=N+M/2,n=M+N/2,k=N)
  right.tail.probs <- phyper(q=0:N-1,m=N+M/2,n=M+N/2,k=N,lower.tail=F)
  tbl <- rbind(left.tail.probs,right.tail.probs)
  rownames(tbl) <- c("P(T <= t)", "P(T >= t)")
  colnames(tbl) <- c("t = 0", 1:N )
  print(tbl)
  @
  So if we set up the rule, ``reject if $T \not\in [3,7]$'', then we'll incorrectly reject with probability \Sexpr{(tbl[1,3] + tbl[2,9])*100}\%. Note that we didn't have to take a region that was centered about any point, and that for a particular level, $\alpha$, there may be many regions that work.
\end{proof}

\paragraph{\#11.6.27} Find the exact null distribution of $W_{+}$ in the case where $n=4$. 
\begin{proof}[Solution]
This problem can be done with pencil and paper, but your best to understand how to have \texttt{R} do it.
  \begin{enumerate}
    \item Like when determining the distribution of any random variable, you should first identify its range. Since $W_{+}$ is the sum of the ranks of the positive differences, it can be anything from 0 to $n(n+1)/2$, where $n$ is the size of your sample. In our case, $n=4$, so the range is $0 \leq W_{+} \leq 10$. 
    \item To get a feel for $P(W_{+} = x)$ for some $x$, we should practice on a few concrete cases. The easy case is $x=0$. Here, this can only happen if all differences are negative, and that happens 1 in $2^4 = 16$ times. Likewise, $x=10$ can only happen if all are positive, so that (also) has probability $1/16$. Now, say $x = 7$. This is ``trickier'', if only because there are multiple ways to take any combination of $\{1,2,3,4\}$ and sum them to 7. E.g. $3 + 4 = 1 + 2 + 4$. Since $n=4$ is small, this isn't much of a doozy. If $n=15$, for instance, then you could have
    \[
     3+4 = 1 + 2 + 4 = 6 + 1 = 5 + 2 = 7
    \]
    which are 5 (you could 7, in this case as a vacuous sum) different ways.
    \item You could continue in this fashion, making sure to find all the ways to make all the numbers between 0 and 10. Or, you can go about this by considering all the ways we can choose $k$ numbers from 4, and summing those various combinations, then tabling the results. In \texttt{R}:
    <<eval=T>>=
    tbl <- table(unlist(sapply(X=0:4, FUN=function(k){combn(x=1:4,m=k,FUN=sum)})))
    prop.table(tbl)
    @
    And as you can see, this coincides with \texttt{R}'s build in density function for $W_{+}$:
    <<>>=
    dsignrank(x=0:10,n=4)
    @
  \end{enumerate}
\end{proof}

\paragraph{\#13.8.9} Given the data presented, was Austen consistent in these habits of style from one work to another? Did her imitator successfully copy this aspect of her style?
\begin{proof}[Solution] 
<<prob13.8.9,echo=F>>=
word.data <- cbind(c(14,133,12,241,11,259),c(16,180,14,285,6,265),c(8,93,12,139,8,221),c(2,81,1,153,17,204))
intra.consistency <- apply(X=combn(x=1:3,m=2),MARGIN=2,FUN=function(pair){
  out <- list(chisq.test(x=word.data[,pair],correct=F,simulate.p.value=F))
  return(out)
})
@
To answer the question of consistency, we'll investigate whether there is homogeneity across various pairs of columns of the data that correspond to works authored by Jane Austen. Doing so, we find that between the first two  and the first and last columns, the tests for homogeneity have an insignificant $p$-values of \Sexpr{intra.consistency[[1]][[1]]$p.value} and \Sexpr{intra.consistency[[2]][[1]]$p.value}. However, it may be safe to reject the hypothesis of consistency between the second and third columns, as the chi-squared test yields a $p$-value of \Sexpr{intra.consistency[[3]][[1]]$p.value}, which is fairly significant.

<<something,echo=FALSE>>=
word.data <- cbind(word.data,rowSums(x=word.data[,1:3]))
inter.consistency <- sapply(X=c(1:3,5),FUN=function(col){
  list(chisq.test(x=word.data[,c(col,4)],correct=F,simulate.p.value=F))
})
p.values <- unlist(lapply(X=inter.consistency,FUN=function(x){x$p.value}))
@
If we compare the imitator's work to the individual preceding titles, we observe p-values of (\Sexpr{p.values[-4]}). So, the author failed to imitate any one book's style, but failed less terribly with \textit{Sandition} I. If we pool all of Austen's stats together to get a more holistic sense of her style, and compare the imitator to that, we get a $p$-value of \Sexpr{p.values[4]} which is still fairly significant.

Basically, using these features of Austen's writing (alone) do not give us much reason to believe the imitator captured Austen's style. Then again, it seemed that Austen failed to capture Austen's style. So, it stands to reason that these features are not very good at identifying any one particular author.
\end{proof}
\end{document}