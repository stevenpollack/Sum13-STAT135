\documentclass[12pt]{article}

\usepackage{amsmath,amssymb,fancyhdr,palatino}
\usepackage[textheight=8in,textwidth=6.5in]{geometry}

\lhead{STAT 135}
\chead{Demonstration \#1}
\rhead{Survey Sampling}
\cfoot{\thepage}

\newcommand{\R}{\texttt{R}}
\newcommand{\N}{\ensuremath{\mathcal{N}}}

\DeclareMathOperator{\var}{var}

\begin{document}

<<globalParameters,echo=FALSE>>=
set.seed(1234)
opts_chunk$set(comment="",tidy=F)
@

\pagestyle{fancy}

\paragraph{The Basics:}
Just like with any language, before we can do anything \textit{remotely} complicated, we need to make sure we have the basics down. For starters, we should know a few things about \R:

\begin{enumerate}
  \item The primitive object in \R{} is a vector. Explicitly, two variables, $x = 1$, and $y = (1,2)$ are both identical objects to \R{} (they differ only in their ``length''). Check it out:
  <<>>=
  x <- 1
  y <- c(1,2)
  length(x)
  length(y)
  @
  \item \R{} works over vectors in an element-wise fashion. So, if we want to (say) square every component of a vector, we just ``square'' the vector:
  <<>>=
  x <- 2:6
  x
  x^2
  @
  \item Perhaps somewhat unintuitively, functions in \R{} are variables. This is a subtle point so let's just illustrate how to make a simple function, and then call it on some data:
  <<>>=
  addOne <- function(z) {
    return(z+1)
  }

  addOne(1)

  addOne(c(1,2))

  addOne(x)
  @
  
  Notice that \texttt{addOne} added 1 to every component of variable fed to it. If we just wanted to make a function that appends 1 to any other vector, we could do
  
  <<>>=
  appendOne <- function(z) {
    return(c(z,1))
  }

  appendOne(1)

  appendOne(c(1,2))

  appendOne(x)
  
  @
  \item Making matrices and arrays in \R{} is only slightly more involved than making vectors:
  <<>>=
  A <- matrix(data=1:10, nrow=2, ncol=5)
  B <- matrix(data=1:10, nrow=5, ncol=2)
  
  # note that, by default, R loads matrices column first!
  A
  B
@
 Always remember: \R{} considers a matrix just a fancy vector with pointers to column breaks. You can do most everything you could with a vector to a matrix and the results are what you'd expect:
 <<>>=
  A^2
  @
  \item Finally, there are times when you'll only want portions of a vector or matrix. The most straight-forward to do this extraction is like so:
  <<>>=
  x
  x[1]
  x[c(2,4)]
  x[3:5]
@
With multidimensional objects like Matrices, you need to specify the coordinates of the subsection that you want. For example:
<<>>=
A

# take the first element
A[1,1]

# take the first row
A[1,]

# take the third column
A[,3]

# take (1,1), (2,3), and (2,5) entries
x.indices <- c(1,2,2) 
y.indices <- c(1,3,5)
indx <- matrix(c(x.indices,y.indices), ncol=2)
indx
A[indx]
@
\end{enumerate}

\paragraph{A little less basic:}
The power of \R{} isn't just in the fact that it's very ``vector friendly''. \R{} has a full, built-in suite of statistical functions to analyze your data (be it in the form of a vector, matrix, array, etc.). 

For example, we know that a collection of data, $x_1, x_2, \ldots, x_{N}$ has mean given by
\[
  \overline{x}_{N} = \frac{1}{N} \sum_{i=1}^{N} x_i
\]
We could do this manually with \R{}:
<<>>=
x <- 1:100
x.bar1 <- sum(x)/length(x)
x.bar1
@
Or, we could just employ the \texttt{mean} function and have \R{} do all the dirty work:
<<>>=
x.bar2 <- mean(x)
x.bar2
@
Similarly, if the data was a sample, we know that the (sample) variance is
\[
  S^2 = \frac{1}{N-1}\sum_{i=1}^{N}(x_i - \overline{x}_{N})^2
\]
And so while we could do this the long way, it turns out \R{} has a built-in function for this:
<<>>=
sample.var1 <- sum((x-mean(x))^2)/(length(x)-1)
sample.var1

sample.var2 <- var(x)
sample.var2
@
Almost any, common, statistic that you'll encounter has been programmed as a function in \R{}. To look for it use \texttt{??} and your search phrase:
<<>>=
??covariance
@
If you know the name of the function, but need help retrieving the syntax details, or other specifics, use \texttt{?} followed by the function name. For example,
<<>>=
?var
@
Don't forget that google is your friend, and you shouldn't hesitate to consult it, or the \R{} help documents to get a handle on some pre-programmed function or programming problem.

\paragraph{Generating random numbers:} Another selling point for \R{} is the ease in which one can generate random numbers according to various distributions. Table \ref{tab:rvs} gives a non-exhaustive list of distributions you can generate random variables from in \R{}. 

\begin{table}[ht!]
\centering
  \begin{tabular}{c}
  \hline
  Beta($\alpha$,$\beta$) \\
  Binomial($n$, $p$) \\
  Cauchy($\ell$, $s$) \\
  $\chi^{2}(\nu)$ \\
  Exponential($\lambda$) \\
  Gamma($\alpha$,$\lambda$) \\
  Geometric($p$) \\
  Hypergeometric($m$, $n$, $k$) \\
  Normal($\mu$, $\sigma^2$) \\
  Negative Binomial($n$, $p$) \\
  Poisson($\lambda$) \\
  Uniform($a$, $b$) \\
  Weibull($\alpha$,$\beta$) \\
  Wilcox($m$, $n$)\\
  \hline
  \end{tabular}
\caption{examples of readily available distributions in \R{}}
\label{tab:rvs}
\end{table}

The syntax for working with this distributions can be confusing at first, so let's just focus on generating numbers. Say, we want to randomly generate 10 numbers, uniformly, from between 0 and 1:
<<>>=
random.draw <- runif(n=10, min=0, max=1)
random.draw
@
We can easily take a quick, numeric summary of this draw with \texttt{summary}:
<<>>=
summary(random.draw)
@
Or, we may want to look at some figures of the data:
<<out.width="0.5\\textwidth",dev='pdf',fig.align='center'>>=
boxplot(random.draw)
# check out histogram, and density estimation
hist(x=random.draw, breaks=seq(from=0, to=1, length.out=7))
plot(density(random.draw, from=0, to=1))
@

\paragraph{(Simple) Random Sampling:} Finally, we arrive at the point of this demonstration: sampling! To sample, however, we should probably start with a pool of data.

Let's suppose we're working a Quality Assurance job at a lightbulb factory, and management wants to revamp the packaging to let people know the lifespan of their bulbs. In a given week, the factory outputs one million ($10^6$) lightbulbs, so it's pretty clear experimenting on the whole population is out of the question (you're one man for god's sake!).

If, for the sake of the argument, the lightbulbs' life spans followed an exponential distribution with mean life-time of three years, then we'd have a situation that looked like

<<dev='pdf',out.width='0.5\\textwidth',fig.align='center'>>=
population <- rexp(n=10^6, rate=1/3)

summary(population)

hist(population,
     breaks=seq(from=0,to=ceiling(max(population)),length.out=20))
@

And taking a ten-bulb, (simple random) sample of the month's output can be done with \texttt{sample}, like so:

<<>>=
test.batch1 <- sample(x=population, size=10, replace=F)
@

We can take this batch back to the lab, and check out the distribution of the life span's directly:
<<dev='pdf',out.width='0.5\\textwidth',fig.align='center'>>=
test.batch1

summary(test.batch1)

hist(test.batch1,
     breaks=seq(from=0,to=ceiling(max(test.batch1)),by=1))
@

However, our bosses aren't sure how confident they should be in your point estimate. In particular, they don't think writing ``Expected life span of \Sexpr{mean(test.batch1)}!'' makes for chic packaging. So, you might be interested in approximating a confidence interval for them, and you recall (from section 7.3.3, in your copy of Rice) the Central Limit Theorem: for $X_1, X_2, \ldots, X_n$ independent, identically distributed random variables from some distribution,
\[
\dfrac{ \overline{X}_{n} - E(X_1) }{\sigma_{\overline{X}_n}} \approx \N(\mu=0,\sigma=1)
\]
where $\sigma_{\overline{X}_n}$ is the standard error of our sample mean. However, 
\[
\sigma^2_{\overline{X}_n} = \dfrac{\var(X_1)}{n}\left(\dfrac{N-n}{N-1}\right)
\]
which relies on the population variance! Since we don't know it, we estimate it with $s^2_{\overline{X}_n}$, given by
\[
  s^2_{\overline{X}_n} = \frac{N-n}{n(n-1)N}\sum_{i=1}^{n}\left(X_i - \overline{X}_n\right)^2
\]

While all this may seem confusing, it all boils down to the following: a 95\% confidence interval for the mean life span is
\[
\left( \overline{X}_{n} - z_{0.025} s_{\overline{X}_n}, \overline{X}_{n} + z_{0.025} s_{\overline{X}_n} \right)
\]
where $z_{0.025}$ is the value such that for standard normal random variate, $Z$,
\[
P(Z > z_{0.025}) = 2.5\%
\]

What does this mean for your bosses? Let's calculate our confidence interval and find out:
<<>>=
n <- 10; N <- 10^6

X.bar <- mean(test.batch1)

s2 <- (N-n)/(n*(n-1)*N)*sum((test.batch1 - X.bar)^2)

z <- qnorm(p=0.025, lower.tail=F)

confidence.int <- X.bar + c(-1,1)*z*sqrt(s2)
confidence.int
@

Turns out, this confidence interval is pretty wide. Infact, its half-length is \Sexpr{(confidence.int[2]-confidence.int[1])/2}. But, that's okay: management (being what it is) has decided that they'll market the bulbs as having a 2.5 year life span, and let consumers be pleasantly surprised when their bulbs last a bit longer.

In the lab assignment to come, you'll investigate the central limit theorem, as well as the effect of sample size (relative to population size) on the quality of your predictions.

\end{document}